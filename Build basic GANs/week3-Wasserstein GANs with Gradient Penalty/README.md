# Wasserstein GANs with Gradient Penalty

Learn advanced techniques to reduce instances of GAN failure due to imbalances between the generator and discriminator! Implement a WGAN to mitigate unstable training and mode collapse using W-Loss and Lipschitz Continuity enforcement.

### Learning Objectives:

* Examine the cause and effect of an issue in GAN training known as mode collapse.
* Implement a Wasserstein GAN with Gradient Penalty to remedy mode collapse.
* Understand the motivation and condition needed for Wasserstein-Loss.

### Reading:

* [Protein GAN](https://www.biorxiv.org/content/10.1101/789719v2)

* [Protein GAN Notebook](https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/ProteinGAN.ipynb)

* Wasserstein GAN (Arjovsky, Chintala, and Bottou, 2017): https://arxiv.org/abs/1701.07875

* Improved Training of Wasserstein GANs (Gulrajani et al., 2017): https://arxiv.org/abs/1704.00028


* From GAN to WGAN (Weng, 2017): https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html

* MNIST Database: http://yann.lecun.com/exdb/mnist/
